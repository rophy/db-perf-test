# YugabyteDB Benchmark Suite - Default Values

# Override chart fullname (default: <release>-yb-benchmark)
fullnameOverride: ""

# Sysbench configuration
# Parameters follow YugabyteDB docs: https://docs.yugabyte.com/stable/benchmark/sysbench-ysql/
sysbench:
  enabled: true
  image:
    repository: rophy/sysbench
    tag: yugabyte-20260119
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2"
      memory: "2Gi"
  # Schedule on worker nodes
  nodeSelector:
    role: worker

  # Database connection
  db:
    host: yb-tserver-service
    port: 5433
    user: yugabyte
    password: yugabyte
    name: yugabyte

  # Benchmark parameters (per YugabyteDB docs)
  tables: 20
  tableSize: 5000000
  threads: 60
  time: 1800
  warmupTime: 300
  reportInterval: 10
  workload: oltp_read_write

  # YugabyteDB-specific flags
  rangeKeyPartitioning: false
  serialCacheSize: 1000
  createSecondary: true

  # Workload tuning flags (CRITICAL: rangeSelects=false prevents 100x slowdown)
  rangeSelects: false
  pointSelects: 10
  indexUpdates: 10
  nonIndexUpdates: 10
  numRowsInInsert: 10
  threadInitTimeout: 90

# Prometheus configuration
prometheus:
  enabled: true
  image:
    repository: prom/prometheus
    tag: v2.48.0
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "1"
      memory: "2Gi"
  retention: 7d
  # Schedule on worker nodes
  nodeSelector:
    role: worker

# YugabyteDB subchart configuration
yugabyte:
  enabled: true
  replicas:
    master: 3
    tserver: 3
  resource:
    master:
      requests:
        cpu: "1"
        memory: "4Gi"
      limits:
        cpu: "2"
        memory: "8Gi"
    tserver:
      requests:
        cpu: "2"
        memory: "10Gi"
      limits:
        cpu: null
        memory: null
  storage:
    master:
      storageClass: ""
      size: 10Gi
    tserver:
      storageClass: ""
      size: 50Gi
  enableLoadBalancer: false
  ysql:
    enabled: true
    port: 5433
  ycql:
    enabled: false
  metrics:
    enabled: true
  gflags:
    master:
      replication_factor: 3
    tserver:
      yb_num_shards_per_tserver: 4
      ysql_num_shards_per_tserver: 4
  # Schedule masters on worker nodes (32GB)
  master:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - worker
  # Schedule tservers on db nodes (16GB each)
  tserver:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - db
